{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flower Recognition Project \n",
    "*PyTorch & Computer Vision for AI Engineering Master with ProfessionAI*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab configuration\n",
    "\n",
    "If you're testing this project on Google Colab, it could be useful for you to run the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this code on Google Colab, run this:\n",
    "\n",
    "!git clone https://github.com/Silvano315/PyTorch-CNN-for-food-image-classification-system.git\n",
    "\n",
    "%cd PyTorch-CNN-for-food-image-classification-system\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp -r /content/drive/MyDrive/Project_PyTorch_ProfessionAI/dataset/ /content/PyTorch-CNN-for-food-image-classification-system/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and initial Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from src.constants import RANDOM_SEED, DATA_PATH, BATCH_SIZE\n",
    "from src.utils import extract_dataset, get_paths_to_files, get_dataset_paths, get_logger\n",
    "from src.visualization import display_random_images, visualize_class_samples, plot_class_distribution, compare_class_distribution, \\\n",
    "                        analyze_image_dimensions, analyze_color_distribution, visualize_random_images, visualize_augmented_images, \\\n",
    "                        scatter_plot_metrics, plot_confusion_matrix \n",
    "from src.preprocessing import create_datasets, create_data_loaders\n",
    "from src.models import create_model, Experiment, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, train_model, validate, \\\n",
    "                        get_predictions, freeze_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15e9ad950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Random Seed for reproducibility\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This repository is connected to CPU\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set it as device \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"This repository is connected to {str(device).upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract zipped Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting files: 100%|██████████| 3674/3674 [00:00<00:00, 5980.24file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to dataset\n",
      "Total files extracted: 3674\n",
      "Removing macOS hidden files...\n",
      "Removed 1841 macOS hidden files/folders\n",
      "Final number of files: 1833\n"
     ]
    }
   ],
   "source": [
    "# Extract Dataset and save files to dataset folder\n",
    "\n",
    "extract_dataset('progetto-finale-flowes.tar.gz', 'dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "File paths:\n",
      "dataset/progetto-finale-flowes/train/daisy/1879567877_8ed2a5faa7_n_jpg.rf.38fa4a22817b8f3b37f50d22ea608f96.jpg\n",
      "dataset/progetto-finale-flowes/train/daisy/7702332000_3f21ef4571_n_jpg.rf.938ac8ed1d43f09d9566cdd327339ddc.jpg\n",
      "dataset/progetto-finale-flowes/train/daisy/2454280137_e1637536ae_n_jpg.rf.f7b21fb60a95a578b5c4f1a880fa1887.jpg\n",
      "dataset/progetto-finale-flowes/train/daisy/12701063955_4840594ea6_n_jpg.rf.058be03c02d95624d8c201fe3eedb333.jpg\n",
      "dataset/progetto-finale-flowes/train/daisy/7630517248_98fb8bee1f_n_jpg.rf.3fa9ef879de7039bf2c346681d49e6e8.jpg\n",
      "================================================================================\n",
      "File names:\n",
      "1879567877_8ed2a5faa7_n_jpg.rf.38fa4a22817b8f3b37f50d22ea608f96.jpg\n",
      "7702332000_3f21ef4571_n_jpg.rf.938ac8ed1d43f09d9566cdd327339ddc.jpg\n",
      "2454280137_e1637536ae_n_jpg.rf.f7b21fb60a95a578b5c4f1a880fa1887.jpg\n",
      "12701063955_4840594ea6_n_jpg.rf.058be03c02d95624d8c201fe3eedb333.jpg\n",
      "7630517248_98fb8bee1f_n_jpg.rf.3fa9ef879de7039bf2c346681d49e6e8.jpg\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get paths and filenames form directory dataset \n",
    "\n",
    "filepaths, filenames = get_paths_to_files(DATA_PATH)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"File paths:\")\n",
    "for i in range(0,5):\n",
    "    print(filepaths[i])\n",
    "print(\"=\"*80)\n",
    "print(\"File names:\")\n",
    "for i in range(0,5):\n",
    "    print(filenames[i])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "File paths:\n",
      "dataset/progetto-finale-flowes/train/daisy/1879567877_8ed2a5faa7_n_jpg.rf.38fa4a22817b8f3b37f50d22ea608f96.jpg\n",
      "dataset/progetto-finale-flowes/train/daisy/7702332000_3f21ef4571_n_jpg.rf.938ac8ed1d43f09d9566cdd327339ddc.jpg\n",
      "dataset/progetto-finale-flowes/train/daisy/2454280137_e1637536ae_n_jpg.rf.f7b21fb60a95a578b5c4f1a880fa1887.jpg\n",
      "dataset/progetto-finale-flowes/train/daisy/12701063955_4840594ea6_n_jpg.rf.058be03c02d95624d8c201fe3eedb333.jpg\n",
      "dataset/progetto-finale-flowes/train/daisy/7630517248_98fb8bee1f_n_jpg.rf.3fa9ef879de7039bf2c346681d49e6e8.jpg\n",
      "================================================================================\n",
      "File names:\n",
      "1879567877_8ed2a5faa7_n_jpg.rf.38fa4a22817b8f3b37f50d22ea608f96.jpg\n",
      "7702332000_3f21ef4571_n_jpg.rf.938ac8ed1d43f09d9566cdd327339ddc.jpg\n",
      "2454280137_e1637536ae_n_jpg.rf.f7b21fb60a95a578b5c4f1a880fa1887.jpg\n",
      "12701063955_4840594ea6_n_jpg.rf.058be03c02d95624d8c201fe3eedb333.jpg\n",
      "7630517248_98fb8bee1f_n_jpg.rf.3fa9ef879de7039bf2c346681d49e6e8.jpg\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Access to path dirs and file names for each split (test, train, val)\n",
    "\n",
    "dataset_paths = get_dataset_paths(DATA_PATH)\n",
    "\n",
    "train_paths, train_names = dataset_paths['train']\n",
    "test_paths, test_names = dataset_paths['test']\n",
    "val_paths, val_names = dataset_paths['valid']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"File paths:\")\n",
    "for i in range(0,5):\n",
    "    print(train_paths[i])\n",
    "print(\"=\"*80)\n",
    "print(\"File names:\")\n",
    "for i in range(0,5):\n",
    "    print(train_names[i])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize n random images \n",
    "\n",
    "fig, axes = display_random_images(filepaths, n=25)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize n random images from a chosen split dataset (train_paths, test_paths, val_paths)\n",
    "\n",
    "chosen_split = train_paths\n",
    "\n",
    "fig, axes = display_random_images(chosen_split, n=25)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays sample images for each class in the chosen split set\n",
    "\n",
    "chosen_split = train_paths\n",
    "\n",
    "fig = visualize_class_samples(chosen_split, num_samples=3, max_classes=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for class distribution for each dataset split\n",
    "\n",
    "chosen_split = train_paths\n",
    "\n",
    "class_dist_fig = plot_class_distribution(chosen_split)\n",
    "class_dist_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the Class Distribution between Train, Test e Validation\n",
    "\n",
    "comparison_fig = compare_class_distribution(dataset_paths)\n",
    "comparison_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several plots to analyze Images Dimensions\n",
    "\n",
    "dim_fig = analyze_image_dimensions(val_paths)\n",
    "dim_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Plots to analyze Colour Distribution for a chosen split dataset\n",
    "\n",
    "chosen_split = train_paths\n",
    "\n",
    "color_dist_fig = analyze_color_distribution(chosen_split, n_samples=50)\n",
    "color_dist_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with image visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Train:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1275\n",
      "    Root location: dataset/progetto-finale-flowes//train\n",
      "    StandardTransform\n",
      "Transform: <src.preprocessing.Transforms object at 0x2a05c3d90>\n",
      "============================================================\\Val:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 364\n",
      "    Root location: dataset/progetto-finale-flowes//valid\n",
      "    StandardTransform\n",
      "Transform: <src.preprocessing.Transforms object at 0x2a0d37bd0>\n",
      "============================================================\\Test:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 182\n",
      "    Root location: dataset/progetto-finale-flowes//test\n",
      "    StandardTransform\n",
      "Transform: <src.preprocessing.Transforms object at 0x2a0d37bd0>\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for Baseline Model (without augmentation, also for train set)\n",
    "# Resize Dimensions (224,224) and Normalization\n",
    "\n",
    "trainset, valset, testset = create_datasets(DATA_PATH, augment_train=False)\n",
    "\n",
    "print('='*60 + \"\\nTrain:\")\n",
    "print(trainset)\n",
    "print('='*60 + \"\\Val:\")\n",
    "print(valset)\n",
    "print('='*60 + \"\\Test:\")\n",
    "print(testset)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of random images from ImageFolder\n",
    "\n",
    "chosen_folder = trainset\n",
    "\n",
    "visualize_random_images(chosen_folder, num_images=16, axis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Transfer Learning Model (with augmentation for train set)\n",
    "# Resize Dimensions (224,224) and Normalization\n",
    "\n",
    "trainset_aug, valset, testset = create_datasets(DATA_PATH, augment_train=True)\n",
    "\n",
    "print('='*60 + \"\\nTrain:\")\n",
    "print(trainset_aug)\n",
    "print('='*60 + \"\\Val:\")\n",
    "print(valset)\n",
    "print('='*60 + \"\\Test:\")\n",
    "print(testset)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of random augmented images from ImageFolder\n",
    "\n",
    "visualize_augmented_images(trainset_aug, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for each dataset split\n",
    "\n",
    "is_augmented = False\n",
    "datasets = {\n",
    "    'train': trainset_aug if is_augmented else trainset,\n",
    "    'val': valset,\n",
    "    'test': testset\n",
    "}\n",
    "\n",
    "dataloaders = create_data_loaders(datasets, batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "train_loader = dataloaders['train']\n",
    "val_loader = dataloaders['val']\n",
    "test_loader = dataloaders['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model: transfer learning with timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Transfer Learning Model (with augmentation for train set)\n",
    "# Resize Dimensions (224,224) and Normalization\n",
    "\n",
    "trainset_aug, valset, testset = create_datasets(DATA_PATH, augment_train=True)\n",
    "\n",
    "print('='*60 + \"\\nTrain:\")\n",
    "print(trainset_aug)\n",
    "print('='*60 + \"\\Val:\")\n",
    "print(valset)\n",
    "print('='*60 + \"\\Test:\")\n",
    "print(testset)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for each dataset split\n",
    "\n",
    "is_augmented = True\n",
    "datasets = {\n",
    "    'train': trainset_aug if is_augmented else trainset,\n",
    "    'val': valset,\n",
    "    'test': testset\n",
    "}\n",
    "\n",
    "dataloaders = create_data_loaders(datasets, batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "train_loader = dataloaders['train']\n",
    "val_loader = dataloaders['val']\n",
    "test_loader = dataloaders['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "num_classes = len(trainset_aug.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and  set to device\n",
    "\n",
    "logger = get_logger(ch_log_level=logging.INFO, fh_log_level=logging.DEBUG)\n",
    "\n",
    "model = create_model(num_classes, model_type='efficientnet', efficientnet_version='b0')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
